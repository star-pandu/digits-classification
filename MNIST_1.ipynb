{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dqdWrd5-VIdI",
      "metadata": {
        "id": "dqdWrd5-VIdI"
      },
      "source": [
        "#<H1 align=center> __A STUDY ON PERFORMANCE AND ANALYSIS OF__ *SKLEARN* __MODELS ON__ *DIGITS* __DATASET__\n",
        "<H1 align=right> Contributed By - <b> Rathlavath Pandu </b> </H1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QdH9Tz7g8LIl",
      "metadata": {
        "id": "QdH9Tz7g8LIl"
      },
      "source": [
        "##<H4> <B>Introduction and importing necessary libraries</B></H4>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mmXP8C-V-Spp",
      "metadata": {
        "id": "mmXP8C-V-Spp"
      },
      "source": [
        "<p>Machine Learning or ML in short is a very powerful emerging tool that has revolutionised almost all walks of life. One such application of machine learning lies in the field of Computer Vision which is object detection and recongition.</p>\n",
        "<p>In this notebook, various learning approaches have been discussed and their performance is analysed on a curated dataset for the task of handwritten digits recongnition.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbIfZSEcWR1y",
      "metadata": {
        "id": "fbIfZSEcWR1y"
      },
      "source": [
        "<H4> Libraries used: </H4>\n",
        "<OL>\n",
        "<LI> scikit-learn: scikit-learn or sklearn is the most widely used library for Machine Learning apllications in Python. It features various classification, regression and clustering algorithms such as support-vector machines(SVMs), multi-layer perceptrons(MLP), random forests and k-means.\n",
        "<LI> numpy:  \n",
        "<LI> matplotlib:\n",
        "<LI> seaborn\n",
        "</OL>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4826acc",
      "metadata": {
        "id": "d4826acc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h601FtSM04J5",
      "metadata": {
        "id": "h601FtSM04J5"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -qqqU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o30Z0A-113Pa",
      "metadata": {
        "id": "o30Z0A-113Pa"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.init(project=\"MNIST\",name=\"Run-1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5J5Ipnod7-XC",
      "metadata": {
        "id": "5J5Ipnod7-XC"
      },
      "source": [
        "##<H4> <B>Dataset Analysis </B></H4>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zAOFQ6NLPy-N",
      "metadata": {
        "id": "zAOFQ6NLPy-N"
      },
      "source": [
        "The <b>Digits dataset of scikit-learn library</b> has been selected for analysis in this experiment. It is a dataset containing <b>1797 images of size 8 X 8 pixels</b>. These images are of handwritten digits ranging from 0 to 9."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FGhESMm1RQTV",
      "metadata": {
        "id": "FGhESMm1RQTV"
      },
      "source": [
        "Before starting the analysis, let us first understand the dataset. Understanding the dataset in question is the one of the key tasks for a ML engineer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zRiZol42TZp-",
      "metadata": {
        "id": "zRiZol42TZp-"
      },
      "source": [
        "So first, we will load the dataset. After that let us go through the documentation maintained in 'DESCR' attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zHub8qbC_43c",
      "metadata": {
        "id": "zHub8qbC_43c"
      },
      "outputs": [],
      "source": [
        "#load the dataset\n",
        "digits = load_digits()\n",
        "# Get the documentation\n",
        "print(digits.DESCR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c-e6vcQ5T_t1",
      "metadata": {
        "id": "c-e6vcQ5T_t1"
      },
      "outputs": [],
      "source": [
        "print(type(digits))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yWeGcjIsUKgs",
      "metadata": {
        "id": "yWeGcjIsUKgs"
      },
      "source": [
        "The dataset contains images of 0-9 digits. It also has a 'target_names' attribute that gives the labels that were used to label the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PKxm1PbxAPs5",
      "metadata": {
        "id": "PKxm1PbxAPs5"
      },
      "outputs": [],
      "source": [
        "# target_names is basically a list of numbers from 0 to 9\n",
        "print(digits.target_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iAefP1rNUtU6",
      "metadata": {
        "id": "iAefP1rNUtU6"
      },
      "source": [
        "Similar to the other scikit-learn datasets, this dataset is also built and maintained in a numpy ndarray object. <br>Let us confirm the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VGx_ptXT9Ely",
      "metadata": {
        "id": "VGx_ptXT9Ely"
      },
      "outputs": [],
      "source": [
        "print(type(digits.data))\n",
        "print(type(digits.target))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DpQYwP2BWCud",
      "metadata": {
        "id": "DpQYwP2BWCud"
      },
      "source": [
        "The dataset consists of 1797 images of size 8X8. These images are each stacked in one dimension as a 64 dimensional vector. Hence, <b>the size of the dataset is 1797 X 64</b>. <br>The ground truth labels are maintained in the <b>target vector of size 1797</b>. Each image has a corresponding ground truth label. <b>There are no missing labels in this dataset</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pAsdeSB5VU42",
      "metadata": {
        "id": "pAsdeSB5VU42"
      },
      "outputs": [],
      "source": [
        "print(\"Size of the dataset: \",digits.data.shape)\n",
        "print(\"Size of the target vector: \",digits.target.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uSUStf72FIlz",
      "metadata": {
        "id": "uSUStf72FIlz"
      },
      "source": [
        "The 64 features are the 64 pixels of the images. They are named as 'pixel_r_c' where *r* stands for row number and *c* stands for column number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HvVz8eLhAdOL",
      "metadata": {
        "id": "HvVz8eLhAdOL"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "# It can be observed that the 64 dim feature vector is indeed a 1D stacked version of the corresponding image\n",
        "for feature in digits.feature_names:\n",
        "  print(feature,end = \", \")\n",
        "  count += 1\n",
        "  if(count==8):\n",
        "     print(\"\\n\")\n",
        "     count = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Xm4ibDZg6r5D",
      "metadata": {
        "id": "Xm4ibDZg6r5D"
      },
      "source": [
        "The fact that the 64 dim feature vector is the 1D stacked version of the corresponding image can be furtner confirmed by the code written in following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BWRwdMpo96PL",
      "metadata": {
        "id": "BWRwdMpo96PL"
      },
      "outputs": [],
      "source": [
        "print(digits.images.shape)\n",
        "# It can be observed that the 64 dimensional feature vector is obtained by stacking the corresponding image in 1D\n",
        "(np.reshape(digits.images,(digits.images.shape[0],-1)) == digits.data).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02umkHslXpmh",
      "metadata": {
        "id": "02umkHslXpmh"
      },
      "source": [
        "<B> SUMMARY</B>\n",
        "1. Number of images in dataset=1797.\n",
        "2. Total number of labels=10 ( 0 to 9)\n",
        "3. The images being 8X8, 64 different features are considered per image( each pixel is considered as a feature)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yncn8QuQ7r4n",
      "metadata": {
        "id": "yncn8QuQ7r4n"
      },
      "source": [
        "The following code displays the first 100 images of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AXgUsRUk9Oqv",
      "metadata": {
        "id": "AXgUsRUk9Oqv"
      },
      "outputs": [],
      "source": [
        "# Display the first 100 images of the dataset\n",
        "# A subplot is created to display the 100 images.\n",
        "fig,ax=plt.subplots(nrows=10,ncols=10,figsize=(10,10))\n",
        "plt.suptitle('Displaying 100 images',va='bottom',fontweight =\"bold\")\n",
        "for index in range(100):\n",
        "    plt.subplot(10, 10, index+1)\n",
        "    plt.title(\"Label: {}\".format(digits.target[index]),)\n",
        "    plt.imshow(digits.images[index], cmap='gray_r')\n",
        "    plt.axis('off')\n",
        "plt.subplots_adjust()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qt1A86JPYtN2",
      "metadata": {
        "id": "qt1A86JPYtN2"
      },
      "source": [
        "##<H4> <B>Train Test Split </B></H4>\n",
        "While splitting the dataset into train and test dataset, the sizes of train and test dataset are taken to be 0.7 and 0.3 of the entire dataset respectively.<br>\n",
        "This is implemented using the train_test_split function in sklearn module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3fee084",
      "metadata": {
        "id": "d3fee084"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split as split\n",
        "X_train, X_test, y_train, y_test = split(digits.data,digits.target,test_size=0.2,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f7vAo6D4khQ",
      "metadata": {
        "id": "1f7vAo6D4khQ"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"NAME\" : \"SK-LEARN DIGITS DATASET\",\n",
        "    \"NUMBER OF IMAGES\" : str(digits.images.shape[0]),\n",
        "    \"SIZE OF EACH IMAGE\" : str(digits.images.shape[1:]),\n",
        "    \"SIZE OF FEATURE VECTOR\" : str(np.product(digits.images.shape[1:])),\n",
        "    \"CLASS NAMES\" : \" \".join(digits.target_names.astype(str)),\n",
        "    \"TRAIN TEST SPLIT\" : 0.8\n",
        "}\n",
        "wandb.config.update(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "413GYu8g7wAf",
      "metadata": {
        "id": "413GYu8g7wAf"
      },
      "source": [
        "##<H4> <B>Feature Selection</B></H4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A_xDu0TdpjF2",
      "metadata": {
        "id": "A_xDu0TdpjF2"
      },
      "outputs": [],
      "source": [
        "#from sklearn.feature_selection import chi2,mutual_info_classif,SelectKBest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GzRXlVDTs-Uq",
      "metadata": {
        "id": "GzRXlVDTs-Uq"
      },
      "outputs": [],
      "source": [
        "# p_values = chi2(X_train,y_train)[1]\n",
        "# plt.stem(p_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6GlrjuDtk8xd",
      "metadata": {
        "id": "6GlrjuDtk8xd"
      },
      "outputs": [],
      "source": [
        "# np.where(p_values>0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Kr0APow7Cex8",
      "metadata": {
        "id": "Kr0APow7Cex8"
      },
      "outputs": [],
      "source": [
        "# info_gain = mutual_info_classif(X_train,y_train)\n",
        "# plt.stem(info_gain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qUBHA9wflP2o",
      "metadata": {
        "id": "qUBHA9wflP2o"
      },
      "outputs": [],
      "source": [
        "# info_gain[np.where(p_values>0.1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ku9sixtAmLcK",
      "metadata": {
        "id": "ku9sixtAmLcK"
      },
      "outputs": [],
      "source": [
        "# np.where(info_gain<0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1WDz_7R-qxQZ",
      "metadata": {
        "id": "1WDz_7R-qxQZ"
      },
      "outputs": [],
      "source": [
        "# X_train = np.delete(X_train,np.where(p_values>0.1),1)\n",
        "# X_test = np.delete(X_test,np.where(p_values>0.1),1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f3_JOavZqF1",
      "metadata": {
        "id": "7f3_JOavZqF1"
      },
      "source": [
        "##<H4> <B>Feature Scaling </B></H4>\n",
        "Feature scaling is implemented using the StandardScaler class.<br>\n",
        "StandardScaler() normalises input x according to the following relationship -<br>\n",
        "$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$\n",
        "z = (x - u) / s<br>\n",
        "where,<br> u is the mean of the training samples if with_mean=True (default option) or zero if with_mean=False<br> and s is the standard deviation of the training samples if with_std=True (default option) or one if with_std=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d19dccdb",
      "metadata": {
        "id": "d19dccdb"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled=scaler.fit_transform(X_train,y_train)\n",
        "X_test_scaled=scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1PHszkPoidBr",
      "metadata": {
        "id": "1PHszkPoidBr"
      },
      "source": [
        "A very useful tool to check accuracy, precision and recall is the <b>confusion matrix.</B><br>\n",
        "By definition a confusion matrix $C$ is such that $C_{ij}$ is equal to the number of observations known to be in group $i$ and predicted to be in group $j$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bQSnaAmsVUh_",
      "metadata": {
        "id": "bQSnaAmsVUh_"
      },
      "source": [
        "Confusion matrix with predicted labels as the ground truth labels gives the distribution of the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23408d53",
      "metadata": {
        "id": "23408d53"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "conf_mat = metrics.confusion_matrix(y_test,y_test)\n",
        "print(conf_mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UmXt3Ee1B0u9",
      "metadata": {
        "id": "UmXt3Ee1B0u9"
      },
      "outputs": [],
      "source": [
        "occur_array = np.unique(y_test, return_counts=True)[1]\n",
        "freq_dict = {idx: occur_array[idx] for idx in range(10)}\n",
        "print(freq_dict)\n",
        "(np.diag(conf_mat) == occur_array).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yc5BSebPV_G2",
      "metadata": {
        "id": "Yc5BSebPV_G2"
      },
      "source": [
        "Now that we are confortable with the dataset, we are ready to train various classification models defined in the sklearn library itself."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AIdloegFWW2_",
      "metadata": {
        "id": "AIdloegFWW2_"
      },
      "source": [
        "#<H1 align=center> __A STUDY ON PERFORMANCE AND ANALYSIS OF__ *SKLEARN* __MODELS ON__ *DIGITS* __DATASET__<br>\n",
        "<H1 align=center> <b>PART-2 EXPERIMENTATION USING SKLEARN MODELS </H1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cpwt2L096myr",
      "metadata": {
        "id": "cpwt2L096myr"
      },
      "source": [
        "##<H2> K-Nearest Neighbour</H2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MvXD5BTJb1-L",
      "metadata": {
        "id": "MvXD5BTJb1-L"
      },
      "source": [
        "Let us first try with a simple algorithm - [Nearest Neighbour Classification](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification) <br>\n",
        "Advantage -\n",
        "Due to it being non-parametric, often successful when decision boundary is very irregular."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sKXDaxyYF_F2",
      "metadata": {
        "id": "sKXDaxyYF_F2"
      },
      "source": [
        "Following the analysis done [here](https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py) the decision regions are plotted for three of the classes compared together.\n",
        "We will consider the following two sets of classes - <br>\n",
        "<OL>\n",
        "<LI> (5,6,8)\n",
        "<LI> (4,9,3)\n",
        "</OL>  We will also plot the decision regions for all the classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3yg2CVzgrZq",
      "metadata": {
        "id": "e3yg2CVzgrZq"
      },
      "outputs": [],
      "source": [
        "from sklearn import neighbors, datasets\n",
        "from sklearn.inspection import DecisionBoundaryDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rHsknbAIGehY",
      "metadata": {
        "id": "rHsknbAIGehY"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import KernelPCA\n",
        "pca = KernelPCA(n_components=2,kernel = 'rbf')\n",
        "x_vis = pca.fit_transform(X_train_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tC7hzV9zHrlB",
      "metadata": {
        "id": "tC7hzV9zHrlB"
      },
      "outputs": [],
      "source": [
        "cmap_light = ListedColormap([\"palegreen\", \"lightcoral\",\"lavender\", \"wheat\",\"thistle\",\"antiquewhite\",\"aquamarine\",\"ivory\",\"cyan\",\"lightpink\"])\n",
        "cmap_bold = [\"chocolate\", \"darkmagenta\", \"red\",\"darkgreen\",\"darkgoldenrod\",\"darkblue\",\"darkslategray\",\"saddlebrown\",\"darkorange\",\"deeppink\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z_Avxxy9lTUl",
      "metadata": {
        "id": "z_Avxxy9lTUl"
      },
      "outputs": [],
      "source": [
        "for n_neighbors in [3,10,15,20,30,50]:\n",
        "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=\"uniform\")\n",
        "    clf.fit(x_vis,y_train)\n",
        "\n",
        "    _, ax = plt.subplots()\n",
        "    DecisionBoundaryDisplay.from_estimator(clf,tempx,cmap=cmap_light,ax=ax,response_method=\"predict\",plot_method=\"pcolormesh\",shading=\"auto\")\n",
        "\n",
        "    # Plot also the training points\n",
        "    sns.scatterplot(x=x_vis[:, 0],y=x_vis[:, 1],hue=y_train,palette=cmap_bold,alpha=1.0,edgecolor=\"black\")\n",
        "    plt.title(\"10-Class classification (k = %i, weights = '%s')\" % (n_neighbors,\"uniform\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n6oGplYZ6hY5",
      "metadata": {
        "id": "n6oGplYZ6hY5"
      },
      "source": [
        "##<H2> PCA</H2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1mZax0pF3HCZ",
      "metadata": {
        "id": "1mZax0pF3HCZ"
      },
      "source": [
        "Let us also implement [Principal Component Analysis](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn-decomposition-pca) on the dataset.<br>\n",
        "We will use [K-Means clustering algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn-cluster-kmeans) after reducing dimension of the data.\n",
        "This is to mitigate the [effect of high dimensionality](https://towardsdatascience.com/the-curse-of-dimensionality-50dc6e49aa1e) on the performance of K-Means model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "izJMg-rSX8Cu",
      "metadata": {
        "id": "izJMg-rSX8Cu"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca2 = PCA(n_components=X_train_scaled.shape[1])\n",
        "x_vis2 = pca2.fit_transform(X_train_scaled)\n",
        "x_vis2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-qeNjz2vIpOo",
      "metadata": {
        "id": "-qeNjz2vIpOo"
      },
      "outputs": [],
      "source": [
        "plt.plot(pca2.explained_variance_ratio_)\n",
        "plt.xticks(list(range(0,X_train_scaled.shape[1],10)) + [X_train_scaled.shape[1] - 1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q6p250-ZMidu",
      "metadata": {
        "id": "q6p250-ZMidu"
      },
      "outputs": [],
      "source": [
        "var_thresh = 0.01\n",
        "np.argmin(pca2.explained_variance_ratio_[pca2.explained_variance_ratio_>var_thresh])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79OI_xeWJj2u",
      "metadata": {
        "id": "79OI_xeWJj2u"
      },
      "outputs": [],
      "source": [
        "first_2 = x_vis[:,:2]\n",
        "first_2_var = pca2.explained_variance_ratio_[:2]\n",
        "pca2 = PCA(n_components=2)\n",
        "x_vis2 = pca2.fit_transform(X_train_scaled)\n",
        "np.allclose(first_2,x_vis2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lQsyOoqtKJTt",
      "metadata": {
        "id": "lQsyOoqtKJTt"
      },
      "outputs": [],
      "source": [
        "print(first_2_var)\n",
        "print(pca2.explained_variance_ratio_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cTPBz5Bp5t19",
      "metadata": {
        "id": "cTPBz5Bp5t19"
      },
      "source": [
        "##<H2>KMeans -</H1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W-SpdR1Y6ir5",
      "metadata": {
        "id": "W-SpdR1Y6ir5"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zxQ6W4dt7wUK",
      "metadata": {
        "id": "zxQ6W4dt7wUK"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import sys\n",
        "# Credits : https://stackoverflow.com/questions/65683128/how-to-plot-the-cost-inertia-values-in-sklearn-kmeans\n",
        "old_stdout = sys.stdout\n",
        "new_stdout = io.StringIO()\n",
        "sys.stdout = new_stdout\n",
        "\n",
        "cls = KMeans(n_clusters = 10,verbose = 3,copy_x = True)\n",
        "out = cls.fit_transform(x_vis2,y_train)\n",
        "printed = new_stdout.getvalue()  #<- store printed output\n",
        "sys.stdout = old_stdout\n",
        "\n",
        "#Extract inertia values\n",
        "inertia_list = []\n",
        "stop = False\n",
        "for i in printed.split('\\n'):\n",
        "  if('inertia' in i):\n",
        "    inertia_list.append(float(i.split('inertia ')[1][:-1]))\n",
        "    stop = True\n",
        "  if(('Initialization' in i) and stop):\n",
        "    break\n",
        "#Plot\n",
        "fig = plt.figure()\n",
        "plt.plot(inertia_list)\n",
        "plt.title(\"Inertia per iteration\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Inertia\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "809AdccwAo7j",
      "metadata": {
        "id": "809AdccwAo7j"
      },
      "outputs": [],
      "source": [
        "wandb.log({\"KMeans Inertia\":fig})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iRTlxE88Tufo",
      "metadata": {
        "id": "iRTlxE88Tufo"
      },
      "outputs": [],
      "source": [
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fLSSGDyyZJWP",
      "metadata": {
        "id": "fLSSGDyyZJWP"
      },
      "outputs": [],
      "source": [
        "h = 0.02 # Step size of the mesh.\n",
        "\n",
        "# Plot the decision boundary.\n",
        "x_min, x_max = x_vis2[:, 0].min() - 1, x_vis2[:, 0].max() + 1\n",
        "y_min, y_max = x_vis2[:, 1].min() - 1, x_vis2[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "# Obtain labels for each point in mesh. Use last trained model.\n",
        "Z = cls.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "# Put the result into a color plot\n",
        "Z = Z.reshape(xx.shape)\n",
        "fig = plt.figure()\n",
        "plt.clf()\n",
        "\n",
        "cmap_light = ListedColormap([\"palegreen\", \"lightcoral\",\"lavender\", \"wheat\",\"thistle\",\"antiquewhite\",\"aquamarine\",\"ivory\",\"cyan\",\"lightpink\"])\n",
        "cmap_bold = [\"chocolate\", \"darkmagenta\", \"red\",\"darkgreen\",\"darkgoldenrod\",\"darkblue\",\"darkslategray\",\"saddlebrown\",\"darkorange\",\"deeppink\"]\n",
        "plt.imshow(\n",
        "    Z,\n",
        "    interpolation=\"nearest\",\n",
        "    extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
        "    cmap=cmap_light,\n",
        "    aspect=\"auto\",\n",
        "    origin=\"lower\",\n",
        ")\n",
        "\n",
        "sns.scatterplot(x = x_vis2[:, 0], y = x_vis2[:, 1],hue = y_train,palette = cmap_bold, size=2)\n",
        "# Plot the centroids as a white X\n",
        "centroids = cls.cluster_centers_\n",
        "plt.scatter(\n",
        "    centroids[:, 0],\n",
        "    centroids[:, 1],\n",
        "    marker=\"x\",\n",
        "    linewidths=3,\n",
        "    color=\"black\",\n",
        "    zorder=10,\n",
        ")\n",
        "plt.title(\n",
        "    \"K-means clustering on the digits dataset (Linear PCA-reduced data)\\n\"\n",
        "    \"Centroids are marked with black cross\"\n",
        ")\n",
        "plt.xlim(x_min, x_max)\n",
        "plt.ylim(y_min, y_max)\n",
        "plt.xticks(())\n",
        "plt.yticks(())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HchkTqqF_SM6",
      "metadata": {
        "id": "HchkTqqF_SM6"
      },
      "outputs": [],
      "source": [
        "wandb.log({\"KMeans Decision Boundary\":fig})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84xuXSUEjXuz",
      "metadata": {
        "id": "84xuXSUEjXuz"
      },
      "outputs": [],
      "source": [
        "wandb.sklearn.plot_elbow_curve(cls,x_vis)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i3rNdrFX4Xcw",
      "metadata": {
        "id": "i3rNdrFX4Xcw"
      },
      "source": [
        "[Good Reference for Silhouette plot](https://towardsdatascience.com/elbow-method-is-not-sufficient-to-find-best-k-in-k-means-clustering-fc820da0631d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fgGthMbokJ-e",
      "metadata": {
        "id": "fgGthMbokJ-e"
      },
      "source": [
        "See Elbow plot [here](https://wandb.ai//kaushal-jadhav/MNIST/reports/undefined-23-06-30-19-18-45---Vmlldzo0Nzc0MDYy?accessToken=dy7inl74y4xypqo5gpe6olvobrd10q25ijodpyqvasfnid47gzswtnir8zg235cj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oq69b2tULcYj",
      "metadata": {
        "id": "oq69b2tULcYj"
      },
      "outputs": [],
      "source": [
        "old_stdout = sys.stdout\n",
        "new_stdout = io.StringIO()\n",
        "sys.stdout = new_stdout\n",
        "\n",
        "out = cls.fit_transform(x_vis,y_train)\n",
        "printed = new_stdout.getvalue()  #<- store printed output\n",
        "sys.stdout = old_stdout\n",
        "\n",
        "#Extract inertia values\n",
        "inertia_list = []\n",
        "stop = False\n",
        "for i in printed.split('\\n'):\n",
        "  if('inertia' in i):\n",
        "    inertia_list.append(float(i.split('inertia ')[1][:-1]))\n",
        "    stop = True\n",
        "  if(('Initialization' in i) and stop):\n",
        "    break\n",
        "#Plot\n",
        "fig = plt.figure()\n",
        "plt.plot(inertia_list)\n",
        "plt.title(\"Inertia per iteration\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Inertia\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z22-QBesM81i",
      "metadata": {
        "id": "Z22-QBesM81i"
      },
      "outputs": [],
      "source": [
        "h = 0.02 # Step size of the mesh.\n",
        "\n",
        "# Plot the decision boundary.\n",
        "x_min, x_max = x_vis[:, 0].min() - 1, x_vis[:, 0].max() + 1\n",
        "y_min, y_max = x_vis[:, 1].min() - 1, x_vis[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "# Obtain labels for each point in mesh. Use last trained model.\n",
        "Z = cls.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "# Put the result into a color plot\n",
        "Z = Z.reshape(xx.shape)\n",
        "fig = plt.figure()\n",
        "plt.clf()\n",
        "\n",
        "cmap_light = ListedColormap([\"palegreen\", \"lightcoral\",\"lavender\", \"wheat\",\"thistle\",\"antiquewhite\",\"aquamarine\",\"ivory\",\"cyan\",\"lightpink\"])\n",
        "cmap_bold = [\"chocolate\", \"darkmagenta\", \"red\",\"darkgreen\",\"darkgoldenrod\",\"darkblue\",\"darkslategray\",\"saddlebrown\",\"darkorange\",\"deeppink\"]\n",
        "plt.imshow(\n",
        "    Z,\n",
        "    interpolation=\"nearest\",\n",
        "    extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
        "    cmap=cmap_light,\n",
        "    aspect=\"auto\",\n",
        "    origin=\"lower\",\n",
        ")\n",
        "\n",
        "sns.scatterplot(x = x_vis[:, 0], y = x_vis[:, 1],hue = y_train,palette = cmap_bold, size=2)\n",
        "# Plot the centroids as a white X\n",
        "centroids = cls.cluster_centers_\n",
        "plt.scatter(\n",
        "    centroids[:, 0],\n",
        "    centroids[:, 1],\n",
        "    marker=\"x\",\n",
        "    linewidths=3,\n",
        "    color=\"black\",\n",
        "    zorder=10,\n",
        ")\n",
        "plt.title(\n",
        "    \"K-means clustering on the digits dataset (Kernel PCA-reduced data)\\n\"\n",
        "    \"Centroids are marked with black cross\"\n",
        ")\n",
        "plt.xlim(x_min, x_max)\n",
        "plt.ylim(y_min, y_max)\n",
        "plt.xticks(())\n",
        "plt.yticks(())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tQGb1-5F6WzH",
      "metadata": {
        "id": "tQGb1-5F6WzH"
      },
      "source": [
        "##<H2> Multi Layer Perceptron </H2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdcJvw7lcDAN",
      "metadata": {
        "id": "fdcJvw7lcDAN"
      },
      "source": [
        "First try-\\\n",
        "Using a Multi Layer Perceptron Classifier.\\\n",
        "It  is a supervised learning algorithm which uses an underlying Neural Network to perform the task of classification.\\\n",
        "It supports multi-class classification (in which a sample can belong to more than one class) by applying Softmax as the output function.For each class, the raw output is processed by the logistic function. Values larger or equal to 0.5 are rounded to 1, otherwise to 0. For a predicted output of a sample, the indices where the value is 1 represents the assigned classes of that sample.\\\n",
        "Optimization of the log-loss function is performed using LBFGS here.\\\n",
        "( LBFGS is an approximation to the [BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm) using limited memory.)\n",
        "The number of hidden layers is chosen as 1000."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6DTt7gN4zuln",
      "metadata": {
        "id": "6DTt7gN4zuln"
      },
      "source": [
        "Good Resource for K-Fold CV - [Here](https://machinelearningmastery.com/k-fold-cross-validation/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9d9fbb7",
      "metadata": {
        "id": "d9d9fbb7"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn.utils._testing import ignore_warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "t_mlp = MLPClassifier(max_iter = 1,random_state = 42)\n",
        "params = {\n",
        "    \"hidden_layer_sizes\":[(20,20),(20,),(10,30,10)],\n",
        "    \"activation\": [\"tanh\",\"relu\"],\n",
        "    \"batch_size\": [20,40],\n",
        "    \"solver\":[\"sgd\",\"adam\"],\n",
        "    \"alpha\": [0.0001,0.01],\n",
        "    \"learning_rate\": [\"constant\",\"adaptive\"],\n",
        "    \"nesterovs_momentum\": [True,False],\n",
        "    \"learning_rate_init\": [0.001,0.01],\n",
        "    \"momentum\": [0.9,0.99]\n",
        "}\n",
        "\n",
        "clf = GridSearchCV(t_mlp,params,cv = 5)\n",
        "\n",
        "# To ignore convergence warnings\n",
        "# Ref - https://stackoverflow.com/questions/53784971/how-to-disable-convergencewarning-using-sklearn\n",
        "\n",
        "@ignore_warnings(category=ConvergenceWarning)\n",
        "def fit(X_train,y_train):\n",
        "  clf.fit(X_train,y_train)\n",
        "\n",
        "fit(X_train_scaled,y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KFXRBceEz7-T",
      "metadata": {
        "id": "KFXRBceEz7-T"
      },
      "outputs": [],
      "source": [
        "print(clf.best_params_)\n",
        "print(clf.cv_results_['mean_test_score'].max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "juKXfdZVOzva",
      "metadata": {
        "id": "juKXfdZVOzva"
      },
      "outputs": [],
      "source": [
        "#from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rGr_X9-KTEiC",
      "metadata": {
        "id": "rGr_X9-KTEiC"
      },
      "outputs": [],
      "source": [
        "classifier=MLPClassifier(max_iter = 100,random_state = 42,**clf.best_params_,early_stopping=True)\n",
        "model=classifier.fit(X_train_scaled,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x8IuUuxnGkSe",
      "metadata": {
        "id": "x8IuUuxnGkSe"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "print(\"Final Loss = \",model.loss_curve_[-1])\n",
        "plt.plot(range(len(model.loss_curve_)),model.loss_curve_)\n",
        "plt.title(\"Train Loss Curve\")\n",
        "plt.xlabel(\"Number of Iterations\")\n",
        "plt.ylabel(\"Train Loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xrJZxzEfKdPG",
      "metadata": {
        "id": "xrJZxzEfKdPG"
      },
      "outputs": [],
      "source": [
        "wandb.log({\"MLP Train Loss Curve\":fig})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uIUkf8nsJoIv",
      "metadata": {
        "id": "uIUkf8nsJoIv"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(range(len(model.loss_curve_)),model.validation_scores_)\n",
        "plt.title(\"Validation Loss Curve\")\n",
        "plt.xlabel(\"Number of Iterations\")\n",
        "plt.ylabel(\"Validation Loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5nxEh7oNJsN3",
      "metadata": {
        "id": "5nxEh7oNJsN3"
      },
      "outputs": [],
      "source": [
        "wandb.log({\"MLP Validation Loss Curve\":fig})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dir1fX2kOPzj",
      "metadata": {
        "id": "dir1fX2kOPzj"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy over test dataset = \",model.score(X_test_scaled,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tQgMUwSBGe5u",
      "metadata": {
        "id": "tQgMUwSBGe5u"
      },
      "outputs": [],
      "source": [
        "y_predict = model.predict(X_test_scaled)\n",
        "y_proba = model.predict_proba(X_test_scaled)\n",
        "wandb.sklearn.plot_classifier(model,X_train_scaled, X_test_scaled,y_train, y_test,y_predict,y_proba,range(10),model_name=\"MLP\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bd98e83",
      "metadata": {
        "id": "5bd98e83"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "y_predict=model.predict(X_train_scaled)\n",
        "metrics.accuracy_score(y_train, y_pred=y_predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VoNBaGt2kf6l",
      "metadata": {
        "id": "VoNBaGt2kf6l"
      },
      "outputs": [],
      "source": [
        "prob = model.predict_proba(X_test_scaled)\n",
        "max_prob = np.round(prob.max(axis=1),4)\n",
        "max_prob_idx = prob.argmax(axis=1)\n",
        "\n",
        "from collections import defaultdict\n",
        "max_prob_dict = defaultdict(list)\n",
        "for idx in range(max_prob_idx.shape[0]):\n",
        "  max_prob_dict[max_prob_idx[idx]].append(max_prob[idx])\n",
        "\n",
        "max_prob_dict = {key:np.average(val) for (key,val) in max_prob_dict.items()}\n",
        "max_value = max(list(max_prob_dict.values()))\n",
        "max_prob_dict = {key:val/max_value for (key,val) in max_prob_dict.items()}\n",
        "plt.stem(list(max_prob_dict.keys()),list(max_prob_dict.values()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DDEN2KKn5ffs",
      "metadata": {
        "id": "DDEN2KKn5ffs"
      },
      "source": [
        "##<H2> Random Forrest Classifier </H1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IOJyTLjhlpjd",
      "metadata": {
        "id": "IOJyTLjhlpjd"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier as rfc\n",
        "\n",
        "t_rfc = rfc(random_state = 42)\n",
        "params = {\n",
        "    \"n_estimators\":[100,500,1000],\n",
        "    \"criterion\" : [\"gini\",\"entropy\"]\n",
        "}\n",
        "\n",
        "clf = GridSearchCV(t_rfc,params,cv = 5)\n",
        "clf.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W2HjS_1MoCwt",
      "metadata": {
        "id": "W2HjS_1MoCwt"
      },
      "outputs": [],
      "source": [
        "print(clf.best_params_)\n",
        "print(clf.cv_results_['mean_test_score'].max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04bfd600",
      "metadata": {
        "id": "04bfd600"
      },
      "outputs": [],
      "source": [
        "classifier_2=rfc(random_state=42,**clf.best_params_)\n",
        "model_2=classifier_2.fit(X_train_scaled,y_train)\n",
        "print(\"Accuracy over test dataset = \",model_2.score(X_test_scaled,y_test) )\n",
        "y_predict_2=model_2.predict(X_test_scaled)\n",
        "print(\"Confidence matrix over test dataset:\")\n",
        "conf_mat_pred_2 = metrics.confusion_matrix(y_test,y_predict_2)\n",
        "print(conf_mat_pred_2)\n",
        "print(\"% of Incorrect predictions:\")\n",
        "a = np.diag((conf_mat - conf_mat_pred_2)).astype(np.float64)\n",
        "b = np.diag(conf_mat.astype(np.float64))\n",
        "print(np.round((100*(a/b)),2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XqD8WVGYpp0U",
      "metadata": {
        "id": "XqD8WVGYpp0U"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(model_2.feature_importances_)\n",
        "plt.title(\"Feature Importance\")\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Importance Score\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0TKSD2Ikr7jL",
      "metadata": {
        "id": "0TKSD2Ikr7jL"
      },
      "outputs": [],
      "source": [
        "wandb.log({\"Feature Importance of Random Forest Classifer\":fig})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qihIQlLFr7jU",
      "metadata": {
        "id": "qihIQlLFr7jU"
      },
      "outputs": [],
      "source": [
        "y_predict = model_2.predict(X_test_scaled)\n",
        "y_proba = model_2.predict_proba(X_test_scaled)\n",
        "wandb.sklearn.plot_classifier(model_2,X_train_scaled, X_test_scaled,y_train, y_test,y_predict,y_proba,range(10),model_name=\"RFC\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4z13dyLa3tof",
      "metadata": {
        "id": "4z13dyLa3tof"
      },
      "source": [
        "##<H2>Extra Trees Classifier</H1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NWaojKmqtcyb",
      "metadata": {
        "id": "NWaojKmqtcyb"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier as etc\n",
        "\n",
        "t_etc = etc(random_state = 42)\n",
        "params = {\n",
        "    \"n_estimators\":[100,500,1000],\n",
        "    \"criterion\" : [\"gini\",\"entropy\"]\n",
        "}\n",
        "\n",
        "clf = GridSearchCV(t_etc,params,cv = 5)\n",
        "clf.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IYdjZ_zDtxGw",
      "metadata": {
        "id": "IYdjZ_zDtxGw"
      },
      "outputs": [],
      "source": [
        "print(clf.best_params_)\n",
        "print(clf.cv_results_['mean_test_score'].max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28f3f9c0",
      "metadata": {
        "id": "28f3f9c0",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# from sklearn.ensemble import ExtraTreesClassifier as etc\n",
        "classifier_3=etc(random_state=42,**clf.best_params_)\n",
        "model_3=classifier_3.fit(X_train_scaled,y_train)\n",
        "print(\"Accuracy over test dataset = \",model_3.score(X_test_scaled,y_test) )\n",
        "y_predict_3=model_3.predict(X_test_scaled)\n",
        "print(\"Confidence matrix over test dataset:\")\n",
        "conf_mat_pred_3 = metrics.confusion_matrix(y_test,y_predict_3)\n",
        "print(conf_mat_pred_3)\n",
        "print(\"% of Incorrect predictions:\")\n",
        "a = np.diag((conf_mat - conf_mat_pred_3)).astype(np.float64)\n",
        "b = np.diag(conf_mat.astype(np.float64))\n",
        "print(np.round((100*(a/b)),2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L6apb-zr1jCz",
      "metadata": {
        "id": "L6apb-zr1jCz"
      },
      "outputs": [],
      "source": [
        "y_predict = model_3.predict(X_test_scaled)\n",
        "y_proba = model_3.predict_proba(X_test_scaled)\n",
        "wandb.sklearn.plot_classifier(model_3,X_train_scaled, X_test_scaled,y_train, y_test,y_predict,y_proba,range(10),model_name=\"ETC\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L4yhzqig5yn0",
      "metadata": {
        "id": "L4yhzqig5yn0"
      },
      "source": [
        "##<H2>Support Vectors Classifier</H1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b71ab685",
      "metadata": {
        "id": "b71ab685"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "classifier_4=SVC(C=10,gamma='auto',kernel='rbf')\n",
        "model_4=classifier_4.fit(X_train_scaled,y_train)\n",
        "print(model_4.score(X_test_scaled,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S4HZfGLM9az5",
      "metadata": {
        "id": "S4HZfGLM9az5"
      },
      "outputs": [],
      "source": [
        "y_predict = model_4.predict(X_test_scaled)\n",
        "y_proba = model_4.predict_proba(X_test_scaled)\n",
        "wandb.sklearn.plot_classifier(model_4,X_train_scaled, X_test_scaled,y_train, y_test,y_predict,y_proba,range(10),model_name=\"SVM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gnnkPkbl531e",
      "metadata": {
        "id": "gnnkPkbl531e"
      },
      "source": [
        "##<H2> The Best Approach!</H2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d9ceaed",
      "metadata": {
        "id": "2d9ceaed",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_selection import SelectKBest, VarianceThreshold\n",
        "f_reduced=SelectKBest(k=30)\n",
        "vt=VarianceThreshold(threshold=0.1)\n",
        "X_short=vt.fit_transform(X=X_train_scaled)\n",
        "X_test_short=vt.transform(X_test_scaled)\n",
        "p=make_pipeline(f_reduced,classifier_4)\n",
        "model_5=p.fit(X_short,y_train)\n",
        "print(model_5.score(X_test_short,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2a6eb13",
      "metadata": {
        "id": "a2a6eb13"
      },
      "outputs": [],
      "source": [
        "y_predict_5=model_5.predict(X_test_short)\n",
        "print(metrics.confusion_matrix(y_test,y_predict_5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaxlvXmG9tGF",
      "metadata": {
        "id": "eaxlvXmG9tGF"
      },
      "source": [
        "##<H2>Finally Visualise using T-SNE!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_7OEmDBj_xYD",
      "metadata": {
        "id": "_7OEmDBj_xYD"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def tsne_viz(data, n_components = 2):\n",
        "    tsne = TSNE(n_components = n_components,perplexity=30.0,verbose=1,random_state = 0)\n",
        "    return tsne.fit_transform(data)\n",
        "def plot_representations(data, labels):\n",
        "    fig = plt.figure(figsize = (15, 15))\n",
        "    ax = fig.add_subplot(111)\n",
        "    scatter = ax.scatter(data[:, 0], data[:, 1], c = labels, cmap = 'hsv',label=labels)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "tsne_data = tsne_viz(X_train_scaled)\n",
        "plot_representations(tsne_data,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vQ0H0ag82Xl8",
      "metadata": {
        "id": "vQ0H0ag82Xl8"
      },
      "source": [
        "# TO DO\n",
        "Kernel PCA <br>\n",
        "T-SNE with hyperparameter tuning"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "QdH9Tz7g8LIl",
        "5J5Ipnod7-XC",
        "qt1A86JPYtN2",
        "413GYu8g7wAf",
        "7f3_JOavZqF1",
        "cpwt2L096myr",
        "n6oGplYZ6hY5",
        "cTPBz5Bp5t19",
        "tQGb1-5F6WzH",
        "DDEN2KKn5ffs",
        "4z13dyLa3tof",
        "L4yhzqig5yn0",
        "gnnkPkbl531e",
        "eaxlvXmG9tGF"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
